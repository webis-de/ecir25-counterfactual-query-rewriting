\section{Query Rewriting on Historical Relevance Feedback}

We present three approaches that incorporate historical relevance feedback, from \Ni boosting (can not generalize), over \Nii relevance feedback (might underfit), towards \Niii keyqueries (trade-off under- vs. overfitting). These approaches were previously applied in diverse retrieval scenarios (see Section~\ref{sec:related-work}). However, we are the first to transfer them to a scenario that evolves over time.

For a set $H$~of historical relevance feedback with observations $(q, d, t) \in H$, $rel(q, d, t)$ is the (graded) relevance judgment for document~$d$ for query~$q$ at timestamp~$t$. Document~$d$ might be deleted or substantially changed in the current corpus~(Table~\ref{table-examples}), which is why we counterfactually use the version of the document at the timestamp $t$ where the relevance observation was made.

\paragraph{Boosting Previously Relevant Documents.} Given a ranking~$r$ and a set of historical relevance feedback~$H$, boosting incorporates a documents historical relevance label, no matter how much they have changed or how old the historical relevance feedback is. We apply boosting to increase the score of previously known relevant documents and decrease the score of previously known non-relevant documents. For a document~$d$ previously observed for the query~$q$ at the timestamps $t_{1}, \ldots, t_{k}$, we increase the score if it is relevant, respectively decrease the score if it is not-relevant at the corresponding timestamps using a weighting factor $\lambda^2$ and additionally boost highly relevant or non-relevant documents by a factor of $\mu$, yielding a boosted score of document~$d$ in ranking~$r$ by:
\begin{equation}
\sum\limits_{t \in \{t_{1}, \ldots, t_{k}\}} \lambda^2 \cdot \mu \cdot rel(q,d,t)
\end{equation}
While this qrel boosting is highly effective when documents do not change~\cite{alkhalifa:2024,keller:2024b}, it can not generalize to newly created or deleted documents.

\paragraph{Previously Relevant Documents as Relevance Feedback.} Given a retrieval model, the current document corpus~$D$, and historical relevance feedback~$H$, we expand each query with $k$~terms with the highest tf-idf scores of previously known relevant documents. For a query $q$, the set $D^{+} = \{d| (q,d,t) \in H \wedge rel(q,d,t) > 0\}$ specifies the previously positive documents on which we calculate the tf-idf scores. The top $k$~terms with the highest tf-idf score are obtained for query expansion and appended to the original query. The expanded query is submitted against the retrieval system on the current document corpus $D$ to yield the ranking. As this expansion only uses tf-idf scores on  previous corpora, they can be calculated offline. Improving upon boosting, this allows to generalize from the historical relevance feedback to potentially deleted or newly created documents.


\paragraph{Keyqueries for Previously Relevant Documents.} Given a retrieval model, the current document corpus~$D$, and a set of historical relevance feedback~$H$, we build keyqueries~\cite{froebe:2021c,gollub:2013a,hagen:2016b} against the previously relevant documents. A query $q_{k}$ is a keyquery for the set of target documents $D^{+} = \{d| (q,d,t) \in H \wedge rel(q,d,t) > 0\}$ previously known relevant for a query~$q$ against the corpus $D^{+} \cup D$ for the given retrieval model, iff \Ni every $d \in D^{+}$ is in the top-$k$ results, \Nii $q_{k}$ has more than $l$ results, and \Niii no subquery $q^{'}_{k} \subset q_{k}$ satisfies the above. The first criteria ensures the specificity, the second the generality, and the third one the minimality of the keyquery, together trading off generalizability versus specificity. Traditional relevance feedback does not check the position of feedback documents in the resulting rankings, whereas keyqueries uses them to remove overfitted or underfitted candidates. To generate candidates, we re-implement a previous algorithm~\cite{froebe:2022c} in PyTerrier~\cite{macdonald:2020} that generates candidates from the top-10 RM3 terms. If multiple candidates are keyqueries, we use the one with the highest nDCG@10 on $D^{+}$. Finally, the keyquery is submitted against the corpus $D$ that might, or might not, contain the documents previously known as relevant.

