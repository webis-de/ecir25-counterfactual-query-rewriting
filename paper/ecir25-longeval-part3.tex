\section{Query Rewriting on Historical Relevance Feedback}

We present three approaches that incorporate historical relevance feedback for previously seen queries by increasing maturity, from \Ni boosting (can not generalize), over \Nii relevance feedback (might underfit), towards \Niii keyqueries (trade-off under- vs. overfitting). These three approaches are frequently applied in diverse retrieval scenarios (see Section~\ref{sec:related-work}). However, we are the first to transfer them to a scenario that evolves over time, thereby making them substantially more applicable.

All of our approaches use a set $H$~of historical relevance feedback. For each observation $(q, d, t) \in H$, $rel(q, d, t)$ specifies the (graded) relevance judgment for document~$d$ for query~$q$ at timestamp~$t$, potentially derived via click models. As exemplified in Table~\ref{table-examples}, document~$d$ might be deleted or substantially changed from the current corpus, which is why our relevance feedback and keyquery approaches counterfactually use the version of the document at the previous timestamp $t$ where the relevance observation was made.

\paragraph{Boosting Previously Relevant Documents.} Given a ranking~$r$ obtained by some retrieval system and a set of historical relevance feedback~$H$, the boosting method directly boosts documents by their historical relevance label, no matter how much they have changed or how old the historical relevance feedback is. We apply boosting to increase the score of previously known relevant documents and decrease the score of previously known non-relevant documents. For a document~$d$ previously observed for the query~$q$ at the timestamps $t_{1}, \ldots, t_{k}$, we increase the score if it is relevant, respectively decrease the score if it is not-relevant at the corresponding timestamps using a weighting factor $\lambda^2$ and additionally boost highly relevant or non-relevant documents by a factor of $\mu$, yielding a boosted score of document~$d$ in ranking~$r$ by:
\begin{equation}
\sum\limits_{t \in \{t_{1}, \ldots, t_{k}\}} \lambda^2 \cdot \mu \cdot rel(q,d,t)
\end{equation}

While this qrel boosting is highly effective when documents do not change~\cite{alkhalifa:2024,keller:2024b}, it can not generalize to newly created or deleted documents.


\paragraph{Previously Relevant Documents as Explicit Relevance Feedback.} Given a retrieval model, the current document corpus~$D$, and a set of historical relevance feedback~$H$, we expand each query with the $k$~terms with the highest tf-idf scores of previously known relevant documents for a query. Formally, for a query $q$, the set of $D^{+} = \{d| (q,d,t) \in H \wedge rel(q,d,t) > 0\}$ specifies the previously positive documents on which we calculate the tf-idf scores. The top $k$~terms with the highest tf-idf score are obtained for query expansion and appended to the original query. The expanded query is submitted against the retrieval system on the current document corpus $D$ to yield the ranking. Since the expansion terms solely rely on tf-idf scores based on  previous corpora, they can be efficiently calculated in advance. Improving upon the boosting approach, this approach allows to generalize from the historical relevance feedback to potentially deleted or newly created documents. 


\paragraph{Keyqueries for Previously Relevant Documents.} Given a retrieval model, the current document corpus~$D$, and a set of historical relevance feedback~$H$, we build keyqueries~\cite{froebe:2021c,gollub:2013a,hagen:2016b} against the previously relevant documents. A query $q_{k}$ is a keyquery for the set of target documents $D^{+} = \{d| (q,d,t) \in H \wedge rel(q,d,t) > 0\}$ previously known relevant for a query~$q$ against the corpus $D^{+} \cup D$ for the given retrieval model, iff \Ni every $d \in D^{+}$ is in the top-$k$ results, \Nii $q_{k}$ has more than $l$ results, and \Niii no subquery $q^{'}_{k} \subset q_{k}$ satisfies the above. Out of those keyquery criteria, the first one ensures the specificity, the second one the generality, and the third one the minimality of the keyquery, together allowing to prevent overfitting while being more tailored towards the target documents $D^{+}$. Traditional relevance feedback does not check the position of its feedback documents in the resulting rankings, whereas keyqueries explicitly check query candidates thereby can remove overfitted or underfitted candidates. To generate keyquery candidates, we re-implement a previous algorithm~\cite{froebe:2022d} in PyTerrier~\cite{macdonald:2020} that selects the top-10 terms with RM3 and traverses the power-set of potential queries constructed from those 10~terms. If multiple candidates are keyqueries, we use the one with the highest nDCG@10 against the previously known relevant documents $D^{+}$. Finally, the keyquery is submitted against the corpus $D$ that might, or might not, contain the documents previously known as relevant.

