\section{Query Rewriting on Historical Relevance Feedback}

We present three approaches that incorporate historical relevance feedback, from \Ni boosting (can not generalize), over \Nii relevance feedback (might underfit), towards \Niii keyqueries (trade-off under- vs. overfitting). These approaches were previously applied in diverse retrieval scenarios (see Section~\ref{sec:related-work}). However, we are the first to adapt them to a retrieval scenario that evolves over time.

For a set $H$~of historical relevance feedback with observations $(q, d, t) \in H$, $rel(q, d, t)$ is the (graded) relevance judgment for document~$d$ for query~$q$ at timestamp~$t$. Document~$d$ may have been deleted or substantially changed in the current corpus~(Table~\ref{table-examples}), which is why we counterfactually use the version of the document at the timestamp $t$ where the relevance observation was made.

\paragraph{Boosting Previously Relevant Documents.} Given a ranking~$r$ and a set of historical relevance feedback~$H$, boosting incorporates a document's historical relevance label, regardless of how much the document has changed or how old the relevance feedback is. We apply boosting to adjust the scores of documents based on their historical relevance. For a document~$d$ previously observed for the query~$q$ at the timestamps $t_{1}, \ldots, t_{k}$, we increase the score if it is relevant, respectively decrease the score if it is not-relevant at the corresponding timestamps using a weighting factor $\lambda^2$ and additionally boost highly relevant documents by a factor of $\mu$. The boosted score of document $d$ for query $q$ is then computed by:
% \begin{equation}
% \sum\limits_{t \in \{t_{1}, \ldots, t_{k}\}} \lambda^2 \cdot \mu \cdot rel(q,d,t)
% \end{equation}

\begin{equation}
    \text{score}(q,d) \; = \; \text{score}_0 \;\times \;
    \prod_{t = t_1}^{t_k}
    \begin{cases}
        (1 - \lambda)^2, & \text{if } rel(q,d,t) = 0, \\
        \lambda^2, & \text{if } rel(q,d,t) = 1, \\
        \lambda^2  \mu, & \text{if } rel(q,d,t) = 2.
        \end{cases}
\end{equation}
    
    
While this qrel boosting is highly effective when documents do not change~\cite{alkhalifa:2024,keller:2024b}, it cannot generalize to newly created or deleted documents.

\paragraph{Previously Relevant Documents as Relevance Feedback.} Given a retrieval model, the current document corpus~$D$, and historical relevance feedback~$H$, we expand each query by adding $k$~terms with the highest tf-idf scores of previously known relevant documents. For a query $q$, the set $D^{+} = \{d| (q,d,t) \in H \wedge rel(q,d,t) > 0\}$ specifies the previously positive documents on which we calculate the tf-idf scores. The top $k$~terms with the highest tf-idf scores are obtained for query expansion and appended to the original query. The expanded query is submitted to the retrieval system on the current document corpus $D$ to produce the final ranking. Since this expansion relies solely on tf-idf scores from previous corpora, these scores can be calculated offline. Improving upon boosting, this allows to generalize from the historical relevance feedback to potentially deleted or newly created documents.


\paragraph{Keyqueries for Previously Relevant Documents.} Given a retrieval model, the current document corpus~$D$, and a set of historical relevance feedback~$H$, we construct keyqueries~\cite{froebe:2021c,gollub:2013a,hagen:2016b} against the previously relevant documents. A query $q_{k}$ is a keyquery for the set of target documents $D^{+} = \{d| (q,d,t) \in H \wedge rel(q,d,t) > 0\}$ previously known relevant for a query~$q$ against the corpus $D^{+} \cup D$ for the given retrieval model, iff \Ni every $d \in D^{+}$ is in the top-$k$ results, \Nii $q_{k}$ has more than $l$ results, and \Niii no subquery $q^{'}_{k} \subset q_{k}$ satisfies the above. The first criterion ensures the specificity, the second the generality, and the third the minimality of the keyquery, together trading off generalizability versus specificity. Traditional relevance feedback does not verify the position of feedback documents in the resulting rankings, whereas keyqueries uses them to remove overfitted or underfitted candidates. To generate candidates, we re-implemented a previous algorithm~\cite{froebe:2022c} in PyTerrier~\cite{macdonald:2020} which generates candidates from the top-10 RM3 terms. If multiple candidates are keyqueries, we use the one with the highest nDCG@10 on $D^{+}$. Finally, the keyquery is submitted against the corpus $D$, which may, or may not, contain the documents that were previously relevant.

