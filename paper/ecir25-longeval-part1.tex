\section{Introduction}

Many queries received by a search engine have been seen before~\cite{DBLP:journals/sigir/SilversteinHMM99}. Analyzing how users interact with the results of these queries provides valuable relevance indicators that can improve the search engine's effectiveness, e.g., by constructing click models that synthesize a relevance indicator~\cite{chuklin:2015}. These synthesized labels can be used to boost documents~\cite{keller:2024b}, to train learning-to-rank models~\cite{liu:2011}, or to fine-tune transformer models~\cite{lin:2021}. While the training and fine-tuning of models often require huge amounts of labeled data, boosting also works with few labels~\cite{keller:2024b}. However, it may only be effective shortly after relevance information has been observed, as queries, documents, and relevance may evolve~\cite{keller:2024}. For instance, boosting would fail when documents are deleted or change their content such that they are not relevant to the query anymore (exemplified in Table~\ref{table-examples}).


\input{table-examples}

% TODO: Maybe reframe all our approaches as query rewriting? Or keyqueries on different formulation strategies? E.g., bird song AND boost(doc-1, 5) AND boost(doc-2, 3) AND boost(doc-3, -10) for boosting. The cool thing would be, that it would allow to combine all our approaches into the same query. I.e., one query could use the RF expansions and the boosting at the same time, thereby easily allowing to fully exploit leakage if available while still beeing able to generalize.

To address this challenge, we explore how historical relevance feedback can guide query rewriting approaches. Incorporating the historical relevance feedback into query rewriting works, analogously to boosting, already with few feedback documents~(e.g., RM3~in PyTerrier uses 3~feedback documents as default~\cite{macdonald:2020}). The document corpus naturally evolves over time for various reasons, such as newly created content, updated websites, or the removal of outdated information. While the conventional retrieval setting relies on the factual, current state of the collection. In contrast, we additionally use previous versions of the collection and thereby counterfactually assume that superseded documents remain relevant or are at least suitable for constructing relevance indicators. This counterfactual assumption is motivated by the observation that many document changes are incremental and may not substantially alter their relevance to the original query.

We apply our counterfactual relevance feedback in three scenarios: (1)~via boosting, (2)~via explicit relevance feedback, and (3)~via keyqueries. Boosting re-weights known (non-)relevant documents, which can not generalize to new or deleted documents. Explicit relevance feedback extends the query with terms from known relevant documents but does not test the resulting ranking. Therefore, we use so-called keyqueries~\cite{gollub:2013a} which reformulate queries until the target documents appear in the top positions.

We evaluate our counterfactual query rewriting approaches on the LongEval test collection. Our results show that counterfactual query rewriting is as effective as boosting but generalizes to new documents,  making it more robust than boosting and substantially outperforming neural transformers while being much more efficient as queries can be pre-computed. Our code is publicly available.%
\footnote{\url{https://anonymous.4open.science/r/counterfactual-query-rewriting}}

