\section{Introduction}

Many queries a search engine receives have been seen before~\cite{DBLP:journals/sigir/SilversteinHMM99}. Analyzing how users interact with the results of these queries yields valuable relevance indicators that can improve the search engine's effectiveness, e.g., by constructing click models that synthesize a relevance indicator~\cite{chuklin:2015}. These synthesized labels can be used to boost documents~\cite{keller:2024b}, to train learning-to-rank models~\cite{liu:2011}, or to fine-tune transformer models~\cite{lin:2021}. While the training and fine-tuning of models often require huge amounts of labeled data, boosting also works with few labels~\cite{keller:2024b}. However, boosting might work only shortly after relevance information has been observed, as queries, documents, and relevance may evolve~\cite{keller:2024}. For instance, boosting would fail when documents are deleted or change their content so that they are not relevant to the query anymore (exemplified in Table~\ref{table-examples}).


\input{table-examples}

% TODO: Maybe reframe all our approaches as query rewriting? Or keyqueries on different formulation strategies? E.g., bird song AND boost(doc-1, 5) AND boost(doc-2, 3) AND boost(doc-3, -10) for boosting. The cool thing would be, that it would allow to combine all our approaches into the same query. I.e., one query could use the RF expansions and the boosting at the same time, thereby easily allowing to fully exploit leakage if available while still beeing able to generalize.

To address this challenge, we explore how historical relevance feedback can guide query rewriting approaches. Incorporating the historical relevance feedback into query rewriting works, analogously to boosting, already with few feedback documents~(e.g., RM3~in PyTerrier uses 3~feedback documents as default~\cite{macdonald:2020}). We counterfactually assume that documents that were previously relevant to a query are still part of the retrieval collection. We apply our counterfactual relevance feedback in three scenarios: (1)~via boosting, (2)~via explicit relevance feedback, and (3)~via keyqueries. Boosting re-weighs known (non-)relevant documents, which can not generalize to new or deleted documents. Explicit relevance feedback extends the query with terms from known relevant documents but does not test the resulting ranking. Therefore, we use so-called keyqueries~\cite{gollub:2013a} that reformulate the queries until the target documents appear at the top positions.

We evaluate our counterfactual query rewriting approaches on the LongEval test collection. Our results show that counterfactual query rewriting is as effective as boosting but generalizes to new documents, thereby being more robust than boosting and substantially outperforming neural transformers while being much more efficient as queries can be pre-computed. Our code is publicly available.%
\footnote{\url{https://anonymous.4open.science/r/counterfactual-query-rewriting}}


