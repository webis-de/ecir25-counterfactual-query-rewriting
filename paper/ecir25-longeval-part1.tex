\section{Introduction}

Many of the queries a search engine receives have been seen before~\cite{DBLP:journals/sigir/SilversteinHMM99}. By analyzing how users interact with the displayed results for these queries, valuable relevance information can be gathered. Such signals can be used to improve the search engine's effectiveness, e.g., by constructing click models that synthesize a relevance indicator~\cite{chuklin:2015}. These synthesized labels can be used to boost documents~\cite{keller:2024b}, to train learning-to-rank models~\cite{liu:2011}, or to fine-tune transformer models~\cite{lin:2021}. While the training and fine-tuning of models often require huge amounts of labeled data, boosting also works with few labels~\cite{keller:2024b}. However, boosting might only be applied for a very short time after relevance information have been observed, as queries, documents, and relevance may evolve~\cite{keller:2024}. For instance, boosting would fail when documents are deleted or change their content so that they are not relevant to the query anymore (exemplified in Table~\ref{table-examples}).


\input{table-examples}

To address this challenge, we explore how historical relevance feedback can be incorporated into query rewriting approaches. Incorporating the historical relevance feedback into query rewriting has the advantage that, anaolugously to boosting, already few feedback documents suffice (e.g., RM3~in PyTerrier~\cite{macdonald:2020} uses 3~feedback documents as default). Our approaches counterfactually assume that documents that were previously relevant to a query are still part of the retrieval collection, even when they might be deleted or have changed substantially. Thereby, as long as the indent of the query does not change, our counterfactual query rewriting is robust against updates and deletions of documents, while it can generalize to newly added documents that could not obtain relevance feedback in the past via the rewritten query. We apply our counterfactual relevance feedback in three scenarios, (1)~via boosting, (2)~via explicit relevance feedback, and (3)~via keyqueries. Boosting just moves known relevant documents to the top, and known non-relevant documents to the bottom, which can not generalize to new or deleted documents. Explicit relevance feedback extends the query with terms from known relevant documents. However, the explicit relevance feedback does not incorporate the resulting ranking, therefore, we include the concept of so-called keyqueries~\cite{gollub:2013a} that use the terms suggested by the relevance feedback to reformulate the queries until the target documents appear at the top-positions.

We evaluate our counterfactual query rewriting approaches in the scenario of the LongEval test collection on all available timestamps from July 2022~to August~2023. Our results show that counterfactual query rewriting is as effective as boosting but generalizes to new documents, thereby beeing more robust than boosting and substantially outperforming neural transformers while beeing much more efficient as queries can be pre-computed. Our code is publicly available.%
\footnote{\url{https://anonymous.4open.science/r/counterfactual-query-rewriting}}


