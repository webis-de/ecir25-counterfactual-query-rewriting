\section{Evaluation}

We evaluate our counterfactual query rewriting in the LongEval scenario that comes with overlapping queries across six points in time between June~2022 and August~2023~\cite{alkhalifa:2023,alkhalifa:2024,galuscakova:2023}. We modify the LongEval datasets to focus on queries that re-occur across multiple timestamps to study the effects of evolving documents. We evaluate the retrieval effectiveness of all approaches and use an ablation study to investigate if they generalize beyond previously known relevant documents.

\subsection{Experimental Setup}
The LongEval test collection~\cite{galuscakova:2023} samples documents, queries, and clicks from the French web search engine Qwant. For each timestamp, we remove queries that did not occur at least in one earlier timestamp, leaving 5~time\-stamps for evaluation between July~2022 and August~2023. Figure~\ref{fig:query-overlap} overviews the overlapping queries for the timestamps. For instance, 138~queries from June~2023 re-occur in August~2024, forming the biggest time gap in our evaluation scenario.

We contrast five baselines with our three approaches. We use BM25~\cite{robertson:1994}, BM25 with RM3 expansion (implemented in PyTerrier~\cite{macdonald:2020}), ColBERT~\cite{khattab:2020}, List-in-T5~\cite{tamber:2023}, and monoT5~\cite{nogueira:2020}. We use the default hyperparameters for all baselines (exporting ColBERT, List-in-T5, and monoT5 from TIRA/TIREx~\cite{froebe:2023e,froebe:2023b}). We also implement our three approaches in PyTerrier using BM25 as the underlying retrieval model. For boosting (BM25$_{Boost}$), we set $\lambda=0.7$ and $\mu=2$ based on previous experiments~\cite{keller:2024b}. For relevance feedback (BM25$_{RF}$), $k=10$ feedback terms are used as this is also the default for RM3 in PyTerrier. For keyqueries (BM25$_{keyquery}$), we use 10~feedback terms aiming at queries that retrieve the target tocuments to the top-10 while having more than 25~results.

\begin{figure}[t]
    \begin{minipage}{.48\textwidth}
        \includegraphics[width=\textwidth]{query-overlap}
        \vspace{-4ex}
        \caption{Frequency of overlapping queries over the different timestamps.}
        \label{fig:query-overlap}
    \end{minipage}
    \hfill    
    \begin{minipage}{.50\textwidth}
        \includegraphics[width=\textwidth]{document-similarities}
        \vspace{-4ex}
        \caption{S$_{3}$ Similarities of documents with overlapping URLs as eCDF plot.}
        \label{fig:document-similarities}
    \end{minipage}
\end{figure}

\subsection{Evolution of Documents in the LongEval Corpora over Time}

The documents in the corpora may evolve via deletion, creation, or updates. The corpus comprises between one and 2.5~million documents, with a total 2.6~million created and 1.7~million deleted over time. We measure how the re-occuring documents changed by inspecting their pairwise similarities. We measured the similarity with the $S_{3}$ score~\cite{bernstein:2005} implemented in CopyCat~\cite{froebe:2021a} at default configuration (1~indicates identical, 0~no overlap) as this score aims to identify redundant documents in retrieval scenarios~\cite{bernstein:2005}. Figure~\ref{fig:document-similarities} shows the $S_{3}$ similarities for all document pairs, indicating that 40\,\% do not change their content~($S_{3}=1.0$), whereas around 50\,\% have an $S_{3}$ similarity below~0.8 that indicate non-negligible changes (prior research used 0.82~as near-duplicate threshold on the Web~\cite{froebe:2021a}). Given that the LongEval corpora evolve only slightly, we include an ablation study that removes all overlap to analyze how approaches generalize.



\subsection{Retrieval Effectiveness}
\input{table-results}

We evaluate the effectiveness of our five baselines and our three approaches using nDCG@10. As the relevance labels of the LongEval corpus are derived from click logs, unjudged documents strongly impact the evaluation. In this scenario, it is recommendet to remove unjudged documents~\cite{sakai:2007} which we report as nDCG@10$^{'}$. Table~\ref{tab:table-results} shows the results. ColBERT, List-in-T5, and monoT5 outperform the BM25 baseline in most cases, whreas BM25 with RM3 expansion does not substantially differ from BM25. Our three approaches substantially outperform all five baselines (nDCG$^{'}$ is always higher). After removing the undesired impact of unjudged documents, both BM25$_{RF}$ and BM25$_{keyquery}$ outperform BM25$_{Boost}$, indicating that these approches generalize to newly created or modified documents. Keyqueries are the most effective approach in all cases, outperforming the best transformer by a large margin.

\input{table-results-fold}

We conduct an ablation study to analyze if the improvements of BM25$_{RF}$ and BM25$_{keyquery}$ come from a generalization beyound previously known relevant documents. We remove all documents that occur in previous timestamps from the runs and relevance judgments and evaluate nDCG$^{'}$. This way, all remaining documents have never been seen before. Table~\ref{tab:table-results-fold} shows the results as improvement upon BM25 for our three approaches. As BM25$_{Boost}$ can not generalize to new documents, they never improve (improvement is always~$+0.0$). However, both BM25$_{RF}$ and BM25$_{keyquery}$ generalize to unseen documents.
