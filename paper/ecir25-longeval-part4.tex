\section{Evaluation}

We evaluate our counterfactual query rewriting in the LongEval scenario that comes with overlapping queries across 6~timestamps between June~2022 and August~2023~\cite{alkhalifa:2023,alkhalifa:2024,galuscakova:2023}. First, we modify the LongEval datasets to focus on queries that re-occur across points in time and subsequently study how documents evolve over due to creations, deletions, and content updates. Finally, we evaluate the retrieval effectiveness of all approaches and use an ablation study to investigate if they generalize beyond the previously known relevant documents.

\subsection{Experimental Setup}

% We modify the LongEval corpora~\cite{alkhalifa:2023,alkhalifa:2024,galuscakova:2023} to evaluate retrieval effectiveness for re-occuring queries over time. 
LongEval provides queries, documents, and relevance judgments derived by click models for six months between June~2022 and August~2023. The documents, queries and clicks were sampled from Qwant so that the queries rather do not change their intent over time~\cite{galuscakova:2023} while documents might be updated, deleted, or newly created. For each timestamp, we remove queries that did not occur at least in one earlier timestamp, leaving us with 5~timestamps for evaluation between July~2022 and August~2023~(between 169~and 298~test queries; June~2022 is skipped as no previous logs are available). Figure~\ref{fig:query-overlap} provides an overview of the re-occuring queries between the different timestamps. For instance, 138~queries from June~2023 re-occur in August~2024, forming the biggest time gap in our evaluation scenario, whereas the smallest time gap occurs for the 150~queries from June~2023 that re-occur in July~2023.

In our retrieval experiments, we contrast five baselines with our three approaches. We use BM25~\cite{robertson:1994}, BM25 with RM3 expansion (both implemented in PyTerrier~\cite{macdonald:2020}), ColBERT~\cite{khattab:2020}, List-in-T5~\cite{tamber:2023}, and monoT5~\cite{nogueira:2020}. We keep all the default hyperparameters for all baselines. We also implement our three approaches in PyTerrier using BM25 as the underlying retrieval model. For boosting (BM25$_{Boost}$), we set $\lambda=0.7$ and $\mu=2$ based on previous experiments~\cite{keller:2024b}. For relevance feedback (BM25$_{RF}$), $k=10$ feedback terms are used as this is also the default for RM3 in PyTerrier. For keyqueries (BM25$_{keyquery}$), we use {\color{red} TODO: Hyperparameter}.

%TODO: for camera ready: ColBERT, List-in-T5, and monoT5 baselines come from TIRA/TIREx


\begin{figure}[t]
    \begin{minipage}{.49\textwidth}
        \includegraphics[width=\textwidth]{query-overlap}
        \vspace{-4ex}
        \caption{Frequency of queries over time.}
        \label{fig:query-overlap}
    \end{minipage}
    \hfill    
    \begin{minipage}{.49\textwidth}
        \includegraphics[width=\textwidth]{document-similarities}
        \vspace{-4ex}
        \caption{S$_{3}$ Similarities of documents with overlapping URLs as eCDF plot.}
        \label{fig:document-similarities}
    \end{minipage}
\end{figure}

\subsection{Evolution of Documents in the LongEval Corpora over Time}

The documents in the corpora may evolve via deletion, creation, or updates to the content. Over time, between one and 2.5~million documents were available. Across all timestamps, 1.7~million documents were deleted and 2.6~million documents were newly created. This may also include the re-creation or deletion of documents when gaps in theire history appear. For documents that occur across multiple timestamps, we measure how they changed over time by inspecting their pairwise similarities. We use the $S_{3}$ score~\cite{bernstein:2005} implemented in CopyCat~\cite{froebe:2021a} in default configuration to measure similarity (1~indicates identical documents, 0~documents without any overlap) as this score was specifically developed to identify redundant documents within retrieval scenarios~\cite{bernstein:2005}. Figure~\ref{fig:document-similarities} shows the $S_{3}$ similarities for all documents with overlapping URLs across all timestamps, indicating that 40\,\% of documents do not change their content~($S_{3}=1.0$), whereas around 50\,\% have an $S_{3}$ similarity below~0.8 that indicate non-negligible changes (prior research used 0.82~as near-duplicate threshold on Web corpora~\cite{froebe:2021a}). Given that the LongEval corpora evolve only slightly among the timestamps, we include an ablation study that removes all overlap to analyze how approaches generalize.



\subsection{Retrieval Effectiveness}


\input{table-results}

We evaluate the effectiveness of our five baselines and our three approaches using nDCG@10. However, as the relevance labels of the LongEval corpus are derived from click logs based on unknown rankings, unjudged documents strongly impact the evaluation. In this scenario, it is recommendet to remove unjudged documents to mitigate their impact~\cite{sakai:2007} which we additionally report as nDCG@10$^{'}$. Table~\ref{tab:table-results} shows the results. ColBERT, List-in-T5, and monoT5 outperform the BM25 baseline in most cases, whreas BM25 with RM3 expansion does not substantially differ from BM25. Our three approaches substantially outperform all five baselines (nDCG$^{'}$ is higher than all five baselines across all timestamps). After removing the undesired impact of unjudged documents, both BM25$_{RF}$ and BM25$_{keyquery}$ outperform BM25$_{Boost}$, indicating that these approches generalize to newly created or modified documents. Keyqueries are the most effective approach in all cases, outperforming the best transformer by a large margin.

\input{table-results-fold}

To verify if the improvements of BM25$_{RF}$ and BM25$_{keyquery}$ come, as expected, from a generalization beyound previously known relevant documents, we conduct an ablation study where we remove all documents that occur in previous timestamps from the runs and relevance judgments of each timestamp and evaluate nDCG$^{'}$. This way, all documents in the evaluation have never been seen before. Table~\ref{tab:table-results-fold} shows the results of the ablation study as improvement upon BM25 for our three approaches. As BM25$_{Boost}$ can not generalize to new documents, they never improve (improvement is always~$+0.0$). However, both BM25$_{RF}$ and BM25$_{keyquery}$ generalize as they almost always improve the effectiveness on unseen documents, even significant on Januar~2024.
