\section{Evaluation}

We evaluate our counterfactual query rewriting approaches in the LongEval scenario that comes with overlapping queries accross 6~timestamps between June~2022 and August~2023~\cite{alkhalifa:2023,alkhalifa:2024,galuscakova:2023}. First, we modify the LongEval datasets to focus on queries that re-occur accross timestamps, and subsequently study how documents evolve over time due to deletions and content updates. Finally, we evaluate the retrieval effectiveness of all approaches and use an ablation study to study if they generalize beyound previously known relevant documents.

\subsection{Experimental Setup}

We modify the LongEval corpora~\cite{alkhalifa:2023,alkhalifa:2024,galuscakova:2023} to evaluate retrieval effectiveness for re-occuring queries over time. LongEval provides queries, documents, and relevance judgments derived with click models for June~2022, July~2022, September~2022, January~2023, June~2023, and August~2023. The queries and clicks were sampled from Qwant so that the queries rather do not change their intent over time~\cite{galuscakova:2023} while documents might be updated, deleted, or newly created. For each timestamp, we remove queries that did not occur in an earlier timestamp, leaving us with 5~timestamps for evaluation between July~2022 and August~2023~(between 169~and 298~test queries; June is skipped as no previous logs are available). Figure~\ref{fig:query-overlap} provides an overview of the re-occuring queries between the different timestamps. For instance, 138~queries from June~2023 re-occur in August~2024, forming the biggest time gap in our evaluation scenario, whereas the smallest time gap occurs for the 150~queries from June~2023 that re-occur in July~2023.

In our retrieval experiments, we contrast five baselines with our three approaches. As baselines, we use BM25~\cite{robertson:1994}, BM25 with RM3 expansion (both implemented in PyTerrier~\cite{macdonald:2020}), ColBERT~\cite{khattab:2020}, List-in-T5~\cite{tamber:2023}, and monoT5~\cite{nogueira:2020}. We leave all baselines at their default hyperparameters. We also implement our three approaches in PyTerrier using BM25 as underlying retrieval model. For boosting (BM25$_{Boost}$), we set {\color{red} TODO: Hyperparameter}. For relevance feedback  (BM25$_{RF}$), we use $k=10$ feedback terms as this is the default for RM3 in PyTerrier. For keyqueries (BM25$_{keyquery}$), we use {\color{red} TODO: Hyperparameter}.


%TODO: for camera ready: ColBERT, List-in-T5, and monoT5 baselines come from TIRA/TIREx


\begin{figure}[t]
    \begin{minipage}{.49\textwidth}
        \includegraphics[width=\textwidth]{query-overlap}
        \vspace{-4ex}
        \caption{Frequency of queries and points in time.}
        \label{fig:query-overlap}
    \end{minipage}
    \hfill    
    \begin{minipage}{.49\textwidth}
        \includegraphics[width=\textwidth]{document-similarities}
        \vspace{-4ex}
        \caption{S$_{3}$ Similarities of documents with overlapping URLs as eCDF plot.}
        \label{fig:document-similarities}
    \end{minipage}
\end{figure}

\subsection{Evolution of Documents in the LongEval Corpora over Time}

After we fixed the queries to those that overlap among the different timestamps, only documents may evolve over time. Documents may evolve via deletion, creation, or updates to the content. {\color{red} Accross all timestamps, XY~documents were deleted over time, XY~documents were newly created at some timestamp, and XY~documents were constantly available.} For documents that occur accross multiple timestamps, we measure how they changed over time by inspecting their pairwise similarities. We use the $S_{3}$ score~\cite{bernstein:2005} implemented in CopyCat~\cite{froebe:2021a} in default configuration to measure similarity (1~indicates identical documents, 0~documents without any overlap) as this score was specifically developed to identify redundant documents within retrieval scenarios~\cite{bernstein:2005}. Figure~\ref{fig:document-similarities} shows the $S_{3}$ similarities for all documents with overlapping URLs accross all timestamps, indicating that 40\,\% of documents do not change their content~($S_{3}=1.0$), whereas around 50\,\%have an $S_{3}$ similarity below~0.8 that indicate non-negligible changes (prior research used 0.82~as near-duplicate threshold on Web corpora~\cite{froebe:2021a}). Given that the LongEval corpora evolve only slightly among the timestamps, we include an ablation study that removes all overlap to analyze how approaches generalize.


\subsection{Natural Evolving Test Collection}
In this evaluation scenario, the effectiveness is assessed in a life-like setting where the overlap naturally occurs. Thus, this setting can describe the effectiveness as it may occur in a web search scenario. Currently, the test collection covers six pints in time with, on average, 1.77 million documents. For each point in time between 407 and 1518 queries are logged and on average, 12874 qrels are constructed through the simplified Dynamic Bayesian Network (sDBN) Click Model~\cite{chapelle:2009}. The topic overlap between points in time is displayed in Figure~\ref{fig:query-overlap}, indicating that less than a third of the queries are logged repeatedly.

\input{table-results}

\subsection{Temporal Per Topic Cross Validation}
To emphasize the actual effect of the approaches and how well they generalize to new documents, we evaluated them in a cross-validation setting. For each topic at each point in time, the relevant documents are split into $k$ folds. For each fold, only the test documents are indexed and retrieved. The different approaches can then use the documents in the train split if they were already present at the previous points in time. Since some documents can be relevant for multiple topics, the train test splits must be updated after creation to avoid duplicates. This makes the splits less random and differently large but should be neglectable for this initial evaluation.

\input{table-results-fold}
