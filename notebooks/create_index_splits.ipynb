{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "import yaml\n",
    "import pyterrier as pt\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pz/f66p4s0s11z2c1c1l3v0qbh40000gn/T/ipykernel_25572/1690991193.py:1: DeprecationWarning: Call to deprecated function (or staticmethod) started. (use pt.java.started() instead) -- Deprecated since version 0.11.0.\n",
      "  if not pt.started():\n",
      "Java started and loaded: pyterrier.java, pyterrier.terrier.java [version=5.10 (build: craigm 2024-08-22 17:33), helper_version=0.0.8]\n",
      "/var/folders/pz/f66p4s0s11z2c1c1l3v0qbh40000gn/T/ipykernel_25572/1690991193.py:2: DeprecationWarning: Call to deprecated method pt.init(). Deprecated since version 0.11.0.\n",
      "The following code will have the same effect:\n",
      "pt.java.add_package('com.github.terrierteam', 'terrier-prf', '-SNAPSHOT')\n",
      "pt.java.init() # optional, forces java initialisation\n",
      "  pt.init(boot_packages=[\"com.github.terrierteam:terrier-prf:-SNAPSHOT\"])\n"
     ]
    }
   ],
   "source": [
    "if not pt.started():\n",
    "    pt.init(boot_packages=[\"com.github.terrierteam:terrier-prf:-SNAPSHOT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"../data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(BASE_PATH + \"/LongEval/metadata.yml\", \"r\") as yamlfile:\n",
    "    config = yaml.load(yamlfile, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_itter(sub_collection, fold: set):\n",
    "    documents_path = os.path.join(\n",
    "        BASE_PATH, config[\"subcollections\"][sub_collection][\"documents\"][\"json\"][\"en\"]\n",
    "    )\n",
    "    documents = [\n",
    "        os.path.join(documents_path, path) for path in os.listdir(documents_path)\n",
    "    ]\n",
    "    for doc_split_path in documents:\n",
    "        print(doc_split_path)\n",
    "        with open(doc_split_path, \"r\") as f:\n",
    "            docs = json.load(f)\n",
    "            for doc in docs:\n",
    "                docno = doc[\"id\"]\n",
    "                if docno not in fold:      \n",
    "                    yield {'docno' : docno, 'text' : doc[\"contents\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(sub_collection, split_id, fold):\n",
    "    iter_indexer = pt.IterDictIndexer(BASE_PATH + f\"/index/{sub_collection}_{split_id}\", meta={'docno': 20, 'text': 8096}, verbose=True)\n",
    "    indexref = iter_indexer.index(doc_itter(sub_collection, fold))\n",
    "    return indexref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(sub_collection):\n",
    "    # Test collection\n",
    "    topics = pd.read_csv(BASE_PATH + \"/\" +config[\"subcollections\"][sub_collection][\"topics\"][\"test\"][\"tsv\"][\"en\"], sep=\"\\t\", names=[\"qid\", \"query\"])\n",
    "    qrels = pd.read_csv(BASE_PATH + \"/\" +config[\"subcollections\"][sub_collection][\"qrels\"][\"test\"], sep=\" \", names=[\"qid\", \"Q0\", \"docno\", \"relevance\"])\n",
    "    \n",
    "    # ID maps\n",
    "    docid_map = pd.read_csv('../data/document-groups-relevant.csv.gz', compression='gzip')\n",
    "    docid_map_patch = docid_map[[sub_collection, \"t\"+str(int(sub_collection[-1])+1)]].dropna().set_index(sub_collection).to_dict()[\"t\"+str(int(sub_collection[-1])+1)]\n",
    "\n",
    "    queryid_map = pd.read_csv('../data/query_id_map.csv')\n",
    "    queryid_map = queryid_map[[sub_collection, \"t\"+str(int(sub_collection[-1])+1)]].dropna().set_index(sub_collection).to_dict()[\"t\"+str(int(sub_collection[-1])+1)]\n",
    "\n",
    "    return topics, qrels, docid_map_patch, queryid_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sub_collection(sub_collection, k=3):\n",
    "    overlap = {sub_collection: []}\n",
    "    topics_with_to_few_docs = []\n",
    "    valids_topics = []\n",
    "    \n",
    "    folds = {}\n",
    "    for i in range(0, k):\n",
    "        folds[i] = {\n",
    "            \"train\": set(),\n",
    "            \"test\": set()\n",
    "        }\n",
    "        \n",
    "    # Load data for sub-collection\n",
    "    topics, qrels, docid_map_patch, queryid_map = load_data(sub_collection)\n",
    "    \n",
    "    # Filter relevant documents for logging only\n",
    "    rel_docs = qrels.merge(topics, on=\"qid\")\n",
    "    rel_docs = rel_docs[rel_docs[\"relevance\"] > 0]\n",
    "    print(\"\\nDocs rel for more topics:\", rel_docs.duplicated(subset=[\"docno\"]).sum() , \"/\", len(rel_docs))\n",
    "    \n",
    "    \n",
    "    # Perform splits on topic level\n",
    "    for topic in queryid_map.keys():\n",
    "        # Get relevant documents for topic\n",
    "        rel_docs = qrels.merge(topics, on=\"qid\")\n",
    "        rel_docs = rel_docs[rel_docs[\"relevance\"] > 0]\n",
    "        rel_docs = rel_docs[rel_docs[\"qid\"]==topic]\n",
    "        \n",
    "        overlap[sub_collection].append(len(rel_docs))            \n",
    "        \n",
    "        # If we have fewer relevants docs than k, we skip this topic \n",
    "        if len(rel_docs) < k: \n",
    "            topics_with_to_few_docs.append(topic)\n",
    "            continue\n",
    "        else:\n",
    "            valids_topics.append(topic)\n",
    "\n",
    "        # split\n",
    "        kf = KFold(n_splits=k)\n",
    "        kf.get_n_splits(rel_docs)\n",
    "        for i, (train_index, test_index) in enumerate(kf.split(rel_docs)):\n",
    "            train_ids = rel_docs.iloc[train_index][\"docno\"].to_list()\n",
    "            test_ids = rel_docs.iloc[test_index][\"docno\"].to_list()\n",
    "            \n",
    "            allready_in_train = folds[i][\"train\"].intersection(test_ids)\n",
    "            allready_in_test = folds[i][\"test\"].intersection(train_ids)\n",
    "            \n",
    "            # update folds to ensure each fold has unique documents\n",
    "            processed = set()\n",
    "            for test_id in allready_in_train:\n",
    "                test_ids.remove(test_id)\n",
    "                processed.add(test_id)\n",
    "                \n",
    "                # add a train doc to test if possible to maintain balance\n",
    "                if len(allready_in_test) > 0:\n",
    "                    train_id = allready_in_test.pop()\n",
    "                    train_ids.remove(train_id)\n",
    "                    \n",
    "            allready_in_train -= processed\n",
    "            \n",
    "            # repeat for new train split\n",
    "            for train_id in allready_in_test:\n",
    "                train_ids.remove(train_id)\n",
    "                \n",
    "                # add a test doc to train if possible to maintain balance\n",
    "                if len(allready_in_train) > 0:\n",
    "                    test_id = allready_in_train.pop()\n",
    "                    test_ids.remove(test_id)\n",
    "\n",
    "            folds[i][\"test\"].update(test_ids)\n",
    "            folds[i][\"train\"].update(train_ids)\n",
    "                    \n",
    "\n",
    "    # report on folds\n",
    "    for i in range(0, k):\n",
    "        overlap = len(folds[i][\"train\"].intersection(folds[i][\"test\"])) \n",
    "        train_size = len(folds[i][\"train\"])\n",
    "        test_size = len(folds[i][\"test\"])\n",
    "        ratio = train_size / (train_size + test_size)\n",
    "        print(f\"Fold {i}: train size: {train_size}, test size: {test_size}, overlap: {overlap}, ratio: {ratio:.2f}\")\n",
    "        \n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Docs rel for more topics: 18 / 400\n",
      "Fold 0: train size: 84, test size: 60, overlap: 0, ratio: 0.58\n",
      "Fold 1: train size: 98, test size: 46, overlap: 0, ratio: 0.68\n",
      "Fold 2: train size: 106, test size: 38, overlap: 0, ratio: 0.74\n",
      "\n",
      "Docs rel for more topics: 565 / 3370\n",
      "Fold 0: train size: 500, test size: 338, overlap: 0, ratio: 0.60\n",
      "Fold 1: train size: 570, test size: 268, overlap: 0, ratio: 0.68\n",
      "Fold 2: train size: 606, test size: 232, overlap: 0, ratio: 0.72\n",
      "\n",
      "Docs rel for more topics: 683 / 3835\n",
      "Fold 0: train size: 365, test size: 230, overlap: 0, ratio: 0.61\n",
      "Fold 1: train size: 396, test size: 199, overlap: 0, ratio: 0.67\n",
      "Fold 2: train size: 429, test size: 166, overlap: 0, ratio: 0.72\n",
      "\n",
      "Docs rel for more topics: 1296 / 4362\n",
      "Fold 0: train size: 547, test size: 328, overlap: 0, ratio: 0.63\n",
      "Fold 1: train size: 592, test size: 283, overlap: 0, ratio: 0.68\n",
      "Fold 2: train size: 611, test size: 264, overlap: 0, ratio: 0.70\n",
      "\n",
      "Docs rel for more topics: 580 / 2689\n",
      "Fold 0: train size: 829, test size: 478, overlap: 0, ratio: 0.63\n",
      "Fold 1: train size: 879, test size: 428, overlap: 0, ratio: 0.67\n",
      "Fold 2: train size: 906, test size: 401, overlap: 0, ratio: 0.69\n"
     ]
    }
   ],
   "source": [
    "folds = split_sub_collection(\"t0\", k=3)\n",
    "folds = split_sub_collection(\"t1\", k=3)\n",
    "folds = split_sub_collection(\"t2\", k=3)\n",
    "folds = split_sub_collection(\"t3\", k=3)\n",
    "folds = split_sub_collection(\"t4\", k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
